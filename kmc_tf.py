import pandas as pd
import numpy as np

import keras
from keras.models import Sequential
from keras.layers import Dropout
from keras.layers import Dense
from keras.wrappers.scikit_learn import KerasRegressor

from sklearn.model_selection import cross_val_score
#from sklearn.model_selection import KFold
import sklearn.model_selection as skms
import sklearn.preprocessing as skprep
from sklearn.pipeline import Pipeline

"""
Various neural network archetectures. It seems skl_mlp_model works the best
This script fits kmc data with a tensorflow neural network after the data has been parsed into a
csv generated by kmc_LVgenKrun.py
"""

def baseline_model():
    ffnn = Sequential()
    ffnn.add(Dense(36, input_dim=6, activation='relu'))
    ffnn.add(Dense(36, input_dim=36, activation='relu'))
    ffnn.add(Dense(18, input_dim=36, activation='relu'))
    ffnn.add(Dense(1, input_dim=18, activation='linear'))
    ffnn.compile(loss='mean_squared_error', optimizer='adam', metrics=[])
    return ffnn

def deeper_model():
    ffnn = Sequential()
    ffnn.add(Dense(36, input_dim=6, activation='relu'))
    ffnn.add(Dense(36, input_dim=36, activation='relu'))
    ffnn.add(Dense(36, input_dim=36, activation='relu'))
    ffnn.add(Dense(18, input_dim=36, activation='relu'))
    ffnn.add(Dense(1, input_dim=18, activation='linear'))
    ffnn.compile(loss='mean_squared_error', optimizer='adam', metrics=[])
    return ffnn

def deepest_model_reg():
    ffnn = Sequential()
    ffnn.add(Dense(36, input_dim=6, activation='relu', kernel_regularizer=keras.regularizers.l2(10**-5)))
    ffnn.add(Dense(36, input_dim=36, activation='relu', kernel_regularizer=keras.regularizers.l2(10**-5)))
    ffnn.add(Dense(36, input_dim=36, activation='relu', kernel_regularizer=keras.regularizers.l2(10**-5)))
    ffnn.add(Dense(18, input_dim=36, activation='relu', kernel_regularizer=keras.regularizers.l2(10**-5)))
    ffnn.add(Dense(6, input_dim=18, activation='relu'))
    ffnn.add(Dense(1, input_dim=6, activation='linear'))
    ffnn.compile(loss='mean_squared_error', optimizer='adam', metrics=[])
    return ffnn

def deepest_model():
    ffnn = Sequential()
    ffnn.add(Dense(36, input_dim=6, activation='relu'))
    ffnn.add(Dense(36, input_dim=36, activation='relu'))
    ffnn.add(Dropout(0.5))
    ffnn.add(Dense(36, input_dim=36, activation='relu'))
    ffnn.add(Dropout(0.5))
    ffnn.add(Dense(18, input_dim=36, activation='relu'))
    ffnn.add(Dropout(0.5))
    ffnn.add(Dense(6, input_dim=18, activation='relu'))
    ffnn.add(Dense(1, input_dim=6, activation='linear'))
    ffnn.compile(loss='mean_squared_error', optimizer='adam', metrics=[])
    return ffnn

def poly_model():
    ffnn = Sequential()
    ffnn.add(Dense(84, input_dim=84, activation='relu', kernel_regularizer=keras.regularizers.l1(10**-5)))
    ffnn.add(Dense(42, input_dim=84, activation='relu', kernel_regularizer=keras.regularizers.l1(10**-5)))
    ffnn.add(Dense(1, input_dim=42, activation='linear'))
    ffnn.compile(loss='mean_squared_error', optimizer='adam', metrics=[])
    return ffnn

def skl_mlp_model(al):
    ffnn = Sequential()
    act = "relu"
    #act = "sigmoid"
    ffnn.add(Dense(84, input_dim=6, activation=act, kernel_regularizer=keras.regularizers.l2(al)))
    #ffnn.add(Dense(84, input_dim=84, activation=act, kernel_regularizer=keras.regularizers.l2(al)))
    ffnn.add(Dense(84, input_dim=84, activation=act, kernel_regularizer=keras.regularizers.l2(al)))
    ffnn.add(Dense(42, input_dim=84, activation=act, kernel_regularizer=keras.regularizers.l2(al)))
    ffnn.add(Dense(20, input_dim=42, activation=act, kernel_regularizer=keras.regularizers.l2(al)))
    ffnn.add(Dense(1, input_dim=20, activation='linear'))
    ffnn.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_absolute_error', 'mean_squared_error'])
    return ffnn

def train_test_model(X, Y, model, epoch_num):
    """
    trains tensorflow model on data, and tests using K-cross validation. Prints mean squared error
    Input:
    X (numpy array): feature data
    Y (numpy vector): value to estimate
    model (function): builds and returns tensorflow model
    epoch_num (int): number of epochs to train data on using batch gradient descent
    """
    estimators = []
    estimators.append(('standardize', skprep.StandardScaler()))
    estimators.append(('mlp', KerasRegressor(build_fn=model, epochs=epoch_num, batch_size=50, verbose=0)))
    pipeline = Pipeline(estimators)
    kfold = skms.KFold(n_splits=3, random_state=rn_seed)
    results = cross_val_score(pipeline, X, Y, cv=kfold)
    print("Results: %.10f (%.10f) MSE" % (results.mean(), results.std()))


if __name__ == "__main__":
    rn_seed = 74
    alpha = 10.**-6
    np.random.seed(rn_seed)

    # All LV relevant 3-site terms: [OOA], [AOA], [AOB], [OAB], [AAB], [BAB]


    infile = "/home/nricke/work/tafel/KMC/k_run_prep.csv" #the features in this file are not standardized
    df = pd.read_csv(infile, index_col="Unnamed: 0")

    X = df[["OA+AO","OB+BO","AA","AB+BA","BB", "B3_Estimate"]].values
    Y = df["OAB+BAO"].values

    # standardize training data in a way that can be ported to C
    sc_x = skprep.StandardScaler()
    sc_y = skprep.StandardScaler()
    X_std = sc_x.fit_transform(X)

    # train test split
    X_train, X_test, y_train, y_test = skms.train_test_split(X_std, Y, test_size=0.2, random_state=1)

    checkpoint_path_sk = "training_sk/%s-{epoch:04d}.ckpt" % B3_term
    sk_call = keras.callbacks.ModelCheckpoint(checkpoint_path_sk, save_weights_only=True, verbose=1)

    model_skl = skl_mlp_model(alpha)
    model_skl.save_weights(checkpoint_path_sk.format(epoch=0))
    skl_model_history = model_skl.fit(X_train, y_train, epochs=110, batch_size=10000, callbacks=[sk_call],
                                              validation_data=(X_test, y_test), verbose=2)

    #model_skl.load_weights("training_sk/deep-0120.ckpt") #for loading a pre-trained model
    scores = model_skl.evaluate(X_test, y_test, verbose=2)
    print(scores)
    #model_skl.save("model_skl.h5") # for saving as an hdf5 file that can be ported to C


    ## Other neural network archetecture that didn't work quite as well
    """
    #model_deep = deepest_model()
    #model_deep.save_weights(checkpoint_path_d.format(epoch=0))
    #deepest_model_history = model_deep.fit(X_train, y_train, epochs=50, batch_size=50, callbacks=[d_call],
    #                                          validation_data=(X_test, y_test), verbose=2)
    #
    #model_deep_reg = deepest_model_reg()
    #model_deep_reg.save_weights(checkpoint_path_r.format(epoch=0))
    #deepest_model_history = model_deep_reg.fit(X_train, y_train, epochs=50, batch_size=50, callbacks=[r_call],
    #                                          validation_data=(X_test, y_test), verbose=2)


    #for model in [baseline_model, deeper_model, deepest_model]:
    #    train_test_model(X, Y, model, 10)

    #train_test_model(X, Y, deepest_model, 200)
    #train_test_model(X, Y, deepest_model, 50)
    #train_test_model(X, Y, deepest_model_reg, 50)

    #poly = skprep.PolynomialFeatures(3)
    #X_poly = poly.fit_transform(X)
    #train_test_model(X_poly, Y, poly_model, 15)
    #train_test_model(X_poly, Y, poly_model, 200)
    """
